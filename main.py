import datetime
from logging import config

import pytz
from selenium import webdriver

from ieee import IEEE
from src.spp.types import SPP_document

config.fileConfig('dev.logger.conf')


def driver():
    """
    Selenium web driver
    """
    options = webdriver.ChromeOptions()

    # Параметр для того, чтобы браузер не открывался.
    options.add_argument('headless')

    options.add_argument('window-size=1920x1080')
    options.add_argument("disable-gpu")
    options.add_argument('--start-maximized')
    options.add_argument('disable_infobars')

    return webdriver.Chrome(options)


doc = SPP_document(id=None, title='Learning Cluster Patterns for Abstractive Summarization', abstract=None, text='SECTION I.\nIntroduction\nAs one of the challenging problems of natural language processing, various methods for automatically summarizing text documents have been actively studied so far. Text summarization are mostly divided into extractive and abstractive approaches. In the extractive approach, a summary is generated by extracting and combining a few important sentences from the original text. On the other hand, the abstractive approach reproduces a short text containing important information, rather than taking part of the original text as it is.\nSince abstractive summarization is a relatively more difficult task than extractive summarization because it must understand the meaning of the original text as well as the natural generation of sentences, most studies have focused on extractive summarization solutions. However, recently, starting with the abstractive summarization model using the sequence-to-sequence architecture [1], [2], transformer-based pre-trained models such as Bidirectional Encoder Representations from Transformers (BERT) [3] and Generative Pre-trained Transformer (GPT) [4] have been widely used around text summarization areas.\nA transformer encoder such as BERT is suitable for extractive summarization that selects a few salient sentences from the original text, but not for abstractive summarization generating a concise text. On the other hand, a transformer decoder such as GPT is suitable for summary generation, but cannot learn bidirectional interactions. To clear such hurdles, state-art-of-the studies on abstractive summarization have mainly focused on pre-trained sequence-to-sequence models such as Bidirectional Auto-Regressive Transformers (BART) that combine both transformer encoder and decoder.\nIn the pre-training step, an original text\nd\nis input to both encoder and decoder, in which the encoder is trained with the corrupted text of\nd\n. By learning large amounts of text in a corpus, the encoder-decoder model can develop its ability to distinguish between the original and noised text data. In the fine-tuning step,\nd\nis input to the encoder and the reference summary to the decoder. The encoder transforms sentences in\nd\nto context vectors in the latent semantic space and the decoder learns the summary generation task through the teacher-forcing language model objective, based on the context vectors.\nBecause a well-written abstractive summary is a short text that is rephrased by a few salient sentences in the original text, handling most non-salient sentences is critical in abstractive summarization. In this work, we view the noise information to non-salient sentences in the original text. For effective summary generation, if the decoder can take a look at cluster information e.g., two clusters of salient and non-salient context vectors during fine-tuning, it can attend more over salient context vectors rather than non-salient ones.\nAs shown Figure 1(a), all sentences composing a text document are relevant with each other regardless of their importance so the corresponding context vectors are closely located to each other in the latent semantic space. This mixture of salient and non-salient context vectors makes it difficult to generate a correct summary. In our approach, we view abstractive summarization to the problem of finding a few salient sentences in a document and then concisely rephrasing them while minimizing the impact of non-salient sentences. Specifically, we first assign each sentence in a document to either salient class or non-salient one, and then force the context vectors corresponding to the salient sentences, away from the non-salient ones. As a result, two clusters of salient and non-salient context vectors are formed in the latent space. Such clusters are far apart from each other so noise context vectors are less affected for summary generation. For instance, Figure 1(b) illustrates the result of two clusters formed by the proposed method in which the discriminator identifies a few salient sentences per document and the cluster generator artificially creates two clusters of salient and non-salient context vectors.\nFIGURE 1.\nMain concept of the proposed model. In this figure, suppose that the source text is composed of five sentences. The red and blue points indicate salient and non-salient context vectors from the encoder in BART, respectively. The cluster pattern generation layer transforms a salient context vector\nv\n⃗ \ni\nto another context vector\nv\n⃗ \n′\ni\nthrough\nv\n⃗ \n′\ni\n=w⊙\nv\n⃗ \ni\n, where\nw\nis a certain weight value and\n⊙\nis Hadamard product.\nShow All\nTo take advantage of this cluster information, we propose a novel method of learning cluster patterns of context vectors, based on BART, the best one among pre-trained Transformer sequence-to-sequence models in recent times.1 We call our method ClusterBART (CBART) throughout this article. We also propose (1) cluster normalization using which the decoder learns cluster patterns more robustly and (2) cluster shrinking that reduces the variance between vectors in a cluster and maximizes the margin between two clusters. In particular, note that our method only transforms existing context vector representation\nv\n⃗ \ni\nto new vector representation\nv\n⃗ \n′\ni\n. As a result, binary clusters are formed by these new vectors. To the best of our knowledge, our work is the first study to use cluster patterns of context vectors for summary generation. In XSUM and CNN/DailyMail data sets, the experimental results show that the proposed model improves up to 5% in ROUGE-1/2/L/avg and 0.4% in BERTScore, compared to BART and up to 30% in ROUGE-1/L/avg and 0.8% in BERTScore, compared to PEGASUS.\nThe remainder of this article is organized as follows. In Section II, we introduce existing abstractive summarization methods related to this work. In particular, we discuss the novelty of our method and the main difference between previous studies and our work. In Section III, we describe the details of the proposed method based on the discriminator and the cluster generator. Next, we explain the experimental set-up in Section IV and discuss the experimental results in detail in Section V. Finally, we summarize our current work followed by the future research direction in Section VI.\nSECTION II.\nRelated Work\nFor abstractive summarization, [1] and [2] first proposed the encoder-decoder attention model. Reference [6] improved the neural model of [2] by copying words from an input text through the pointer network, which can handle out-of-vocabulary words, while generating words from the generator network. They also proposed a coverage mechanism that tracks and controls the coverage of input text in order to eliminate repetition.\nWith the great success of the transformer model in natural language generation, [7] and [8] proposed pre-trained models for abstractive summarization. In particular, [7] proposed an unified pre-trained language model (UniLM) that is a multi-layer transformer model as backbone network, jointly pre-trained by various unsupervised language modeling objectives such as (1) bidirectional language model, (2) right-to-left or left-to-right undirectional language model, and sequence-to-sequence language model, sharing the same parameters.\nReference [8] used an encoder-decoder model, in which a document-level encoder using BERT is pre-trained on large amounts of text. They also proposed a new fine-tuning schedule to alleviate the mismatch between the encoder and the decoder with BERTSUMABS and BERTSUMEXTABS, which are the baseline and the two-stage fine-tuned models. Recently, [9] proposed the BART model that consists of BERT as the encoder and GPT as the decoder. The BART model is similar to BERTSUM proposed in [8], except that the encoder can perform the masking task through various denoising functions.\nAs one of the state-of-the-art Transformer encoder-decoder models, PEGASUS uses a new self-supervised pre-training objective named as Gap Sentences Generation (GSG). Unlike BERTSUM and BART, PEGASUS masks an important sentence in a document rather than some token [10]. In fact, this is an extension of the word embedding method to understand the meaning of sentences. For example, in document\nd={\ns\n1\n,\ns\n2\n,\ns\n3\n,\ns\n4\n}\n, where\ns\ni\nis the\ni\n-th sentence in\nd\n. Assuming that\ns\n2\nis the most important sentence,\ns\n2\nis first masked as\ns\n′\n2\n. Then, any token chosen at random from\ns\n1\nsurrounding\ns\n2\nis masked. Similarly, any token chosen at random from\ns\n3\nsurrounding\ns\n2\nis masked. These masked tokens and\ns\n′\n2\nare used as the input of the Transformer encoder. On the other hand,\ns\n2\nis used as the input of the Transformer decoder. In the pre-training step, PEGASUS is learned through the process where the encoder predicts the masked tokens and the decoder generates\ns\n2\ncorrectly.\nBesides these pre-trained models, various models using reinforcement learning [11], [12], topic model [13], [14], multimodal information [15], attention head masking [16], information theory [17], extraction-and-paraphrasing [18], entity aggregation [19], factuality consistency [20], [21], [22], [23], [24], [25], deep communicating agents [26], sentence correspondence [27], graph [28], [29], [30], and bottom-up approach [31] were proposed for abstractive summarization. Because these models are not directly related to our proposed model, we do not compare with them in this article.\nRecently, there have been a few studies utilizing clusters in opinion summarization [32], [33], [34], which is quite different from abstractive summarization. The purpose of opinion summarization is to generate an aspect-based summary that represents opinion popularity well in an unsupervised manner. For opinion summarization, existing methods discover clusters hidden from a collection of customer reviews, using Variational AutoEncoder (VAE), Expectation-Maximization (EM), or supervised aspect classifiers, and then generate aspect-based summaries by averaging the embedding vectors belonging to each cluster. However, our proposed method is different from them. We on purpose create two clusters of salient and non-salient vectors in the semantic latent space, where the cluster of salient context vectors is far from the cluster of non-salient ones, so that the decoder can learn to clearly distinguish between salient and non-salient context vectors for abstractive summarization. In particular, our work on learning artificial cluster patterns in latent semantic space is the first study in abstractive summarization.\nSECTION III.\nMain Proposal\nA. Background: BART\nAbstractive Summarization is based on a language model such as GPT, which learns in the process of predicting the next word with a given word sequence. In this case, words are predicted conditioned on only left-wise context so that it cannot learn in both left-wise and right-wise directions.\nTo address this problem, [3] proposed the masked language model called BERT. Instead of predicting the word after a given sequence, it learns in the process of first informing the model of the entire sequence and then predicting which word corresponds to the (masked) blank. However, there is a disadvantage that it cannot be easily used for summary generation.\nTo improve abstractive summarization, [9] recently proposed BART, a pre-trained sequence-to-sequence model that combines both BERT as the encoder and GPT as the decoder. BERT and GPT are transformers for bidirectional masked and auto-regressive language models, respectively. In particular, BART is a denoising autoencoder model. Specifically, it is trained by first corrupting text with one of arbitrary noising functions such as token masking, token deletion, text infilling, sentence permutation, and document rotation, and then learning the model to reconstruct the original text.\nAs shown in Figure 2, the corrupted text created by one of the noising functions is first encoded with a bidirectional encoder and then a summary is generated with an auto-regressive decoder. For pre-training, the input of the encoder is the corrupted text, while the input of the decoder is the original text. For fine-tuning, the input of the encoder is the original text, while the input of the decoder is the reference summary.\nFIGURE 2.\nOverview of BART [9].\nShow All\nIn BART, the following noising functions are mainly used.\nToken Masking (TM): Some tokens are sampled at random and replaced with [MASK] tokens.\nToken Deletion (TD): Randomly sampled tokens are deleted from the original text.\nToken Infilling (TI): BART does span masking, not token masking in BERT. For example, a text span of three tokens is replaced with a single [MASK] token.\nSentence Permutation (SP): A original text is decomposed into multiple sentences, and then the sentences are shuffled in random order.\nDocument Rotation (DR): After the original text is shuffled, a token chosen at random is appended to the beginning of the text. This trains the model to identify the beginning of the text.\nAccording to Lewis et al.’s experimental result, both TI and SP showed the best performance for text summarization. Thus, we used them as the noising function in our experiments.\nIn addition, Lewis et al. reported that BART outperformed main state-of-the-art summarization models including Lead-3, PTGEN [6], PTGEN+COV [6], UniLM [7], BERTSUMABS [8], and BERTSUMEXTABS [8]. Therefore, we consider BART as the baseline model in this article.\nB. Proposed Method\nOur proposed model for abstractive summarization is based on BART as described in Section III-A. We extend the existing BART model by adding two components: (i) discriminator\nΔ\nand (ii) cluster generator\nτ\n. The pre-training step is the same as the existing BART model. However, the fine-tuning step is quite different. Given an original text as input, the discriminator first splits the original text into a set of sentences and then classifies whether each sentence is salient or not. The cluster generator takes as input context vectors from the encoder in BART and forms two clusters: One is the group of the context vectors corresponding to salient sentences, while the other is the group of the context vectors corresponding to non-salient sentences. The decoder in BART learns these cluster patterns for summary generation and predicts words auto-regressively.\nFigure 3 illustrates the overview of the proposed model for abstractive summarization. The original text document\nd\nconsists of five sentences\nd={\ns\n1\n,\ns\n2\n,\ns\n3\n,\ns\n4\n,\ns\n5\n}\n.\nΔ\ntakes\nd\nas input and classifies whether each sentence is salient or not. For example, if\nΔ\nclassifies\ns\n1\nand\ns\n3\nto be salient, while classifying\ns\n2\n,\ns\n4\n, and\ns\n5\nto be non-salient, we can have two clusters: {\ns\n1\n,\ns\n3\n} and {\ns\n2\n,\ns\n4\n,\ns\n5\n}. The output of the encoder in BART is a set of the context vectors corresponding to the sentences in\nd\n. When\nv\n⃗ \ni\nis a context vector corresponding to sentence\ns\ni\n, we can see two clusters of the context vectors: {\nv\n⃗ \n1\n,\nv\n⃗ \n3\n} and {\nv\n⃗ \n2\n,\nv\n⃗ \n4\n,\nv\n⃗ \n5\n}.\nFIGURE 3.\nOverview of the proposed model (Salient sentences in red and non-salient ones in blue).\nShow All\nBased on this cluster information, the cluster generator converts existing context vectors\nv\n⃗ \n1\n,\nv\n⃗ \n2\n,\nv\n⃗ \n3\n,\nv\n⃗ \n4\n, and\nv\n⃗ \n5\nto new context vectors\nv\n⃗ \n′\n1\n,\nv\n⃗ \n2\n,\nv\n⃗ \n′\n3\n,\nv\n⃗ \n4\n, and\nv\n⃗ \n5\n, satisfying the conditions below:\nv\n⃗ \n′\n1\n≃\nv\n⃗ \n′\n3\n,\nv\n⃗ \n2\n≃\nv\n⃗ \n4\n≃\nv\n⃗ \n5\n, and\n{\nv\n⃗ \n′\n1\n,\nv\n⃗ \n′\n3\n} is far apart from {\nv\n⃗ \n2\n,\nv\n⃗ \n4\n,\nv\n⃗ \n5\n}.\nC. Discriminator\nΔ\nFor\nΔ\n, we use the pre-trained BERT with transformer layers, borrowing similar idea from [8]. Figure 4 shows the neural architecture of\nΔ\nby adding transformer encoders for classification as two layers on top of the existing BERT. The\nΔ\ndivides the input text into sentences and adds a [CLS] token at the beginning of each sentence. Among the token vectors through BERT, only [CLS] token vectors are selected and transfered to the transformer encoders. Finally, the [CLS] token vectors, which contain the representation of each sentence, are output as values of 0 to 1. The higher the output value, the more important the sentence. The sentences with high value are considered to be salient.\nC\np\n,\nC\nn\n←Δ(d,λ)\n(1)\nView Source\nFIGURE 4.\nDiscriminator\nΔ\n.\nShow All\nIn Eq. 1,\nC\np\nis the set of salient sentences, while\nC\nn\nis the set of non-salient sentences. As the input of\nΔ\n,\nλ\nis a hyper-parameter controlling the number of salient sentences in\nd\n. This\nλ\nvalue can be different across various data sets. In our experiments, we find the optimal\nλ\nby investigating ROUGE results in different\nλ\nvalues.\nD. Cluster Generator\nτ\nThe goal of this layer is to transform a context vector\nv\n⃗ \ni\nfrom the encoder neural model in BART to a new vector representation\nv\n⃗ \n′\ni\nincluding clustering information. Please take a look at the following Eq. 2.\nv\n⃗ \n′\ni\n←τ(\nv\n⃗ \ni\n),\n(2)\nView Source\nwhere\nτ()\nis the function about the cluster generator.\nFigure 5(a) depicts four contextualized vectors\nv\n⃗ \n1\n,\nv\n⃗ \n2\n,\nv\n⃗ \n3\n, and\nv\n⃗ \n4\nin the latent space.2 Supposing that a given original text\nd\nconsists of four sentences\ns\n1\n,\ns\n2\n,\ns\n3\n, and\ns\n4\n, e.g.,\nd={\ns\n1\n,\ns\n2\n,\ns\n3\n,\ns\n4\n}\n,\nv\n⃗ \ni\nis the context vector of\ns\ni\nand |\nv\n⃗ \ni\n| is the length of\nv\n⃗ \ni\n. In the figure, the red vectors (e.g.,\nv\n⃗ \n1\nand\nv\n⃗ \n3\n) are lableled as salient sentences from\nΔ\n, while the blue ones (e.g.,\nv\n⃗ \n2\nand\nv\n⃗ \n4\n) as non-salient ones.\nFIGURE 5.\nCluster generator\nτ\n.\nShow All\n1) Cluster Generation\nTo cluster salient and non-salient sentences, the representation of the context vectors corresponding to the salient sentences is recalculated by the following equation.\nv\n⃗ \n(l)\ni\n={\nτ\n1\n(w⋅\nv\n⃗ \ni\n)\nτ\n1\n(\nv\n⃗ \ni\n)\nif \nv\n⃗ \ni\n∈\nC\np\nif \nv\n⃗ \ni\n∈\nC\nn\n,\nView Source\nwhere\nw\nis a certain weight value that is a hyper-parameter and we used the optimal\nw\nthrough a series of experiments with different\nw\nvalues. As shown in Figure 5(b), when\nw=−1\n,\nv\n⃗ \n(l)\ni\nis located in the opposite direction of\nv\n⃗ \ni\nin the latent space. In the figure, context vectors corresponding to non-salient sentences exist in the first quadrant of latent space coordinates, and ones corresponding to salient sentences move to the third quadrant. As a result, two clusters can be formed in the latent space. Now vectors\nv\n⃗ \n(l)\n1\nand\nv\n⃗ \n(l)\n3\nare changed to new vector representations containing clustering information.\n2) Cluster Normalization\nIn addition,\nC\n′\np\n={\nv\n⃗ \n(l)\n1\n,\nv\n⃗ \n(l)\n3\n}\ncan be further normalized into\nC\n′′\np\n={\nv\n⃗ \n(l+1)\n1\n,\nv\n⃗ \n(l+1)\n3\n}\nusing Eq. 3 below.\nv\n⃗ \n(l+1)\ni\n=\nτ\n2\n(\nv\n⃗ \n(l)\ni\n−\nv\n⃗ \nμ\np\nσ\n),\n(3)\nView Source\nwhere\nv\n⃗ \nμ\np\nis the mean vector, shown as the black vector in Figure 5(c), and\nσ\nis the standard deviation in\nC\n′\np\n.\nC\n′\nn\n={\nv\n⃗ \n(l)\n2\n,\nv\n⃗ \n(l)\n4\n}\nis also normalized into\nC\n′′\nn\n={\nv\n⃗ \n(l+1)\n2\n,\nv\n⃗ \n(l+1)\n4\n}\n. We expect that transformation of the vector representation through Eq. 3 will enable the decoder model in BART to learn cluster patterns more robustly.\n3) Cluster Shrinking\nOur another approach is to forcibly shrink the size of clusters (\nC\n′\np\nand\nC\n′\nn\n) that have already been created by Eq. 3. We first obtain the mean vectors\nv\n⃗ \nμ\np\nin\nC\n′\np\nand\nv\n⃗ \nμ\nn\nin\nC\n′\nn\n, respectively. Next, for each cluster (e.g.,\nC\n′\np\n), we compute Euclidean distances between\nv\n⃗ \nμ\np\nand\nv\n⃗ \n(l+1)\ni\n∈\nC\n′\np\n. For instance, after the distance\ndist(\nv\n⃗ \nμ\np\n,\nv\n⃗ \n(l+1)\n3\n)\nis computed, it is indicated by the dotted line in Figure 5(d). Finally, we divide the distance by the ratio\nm\nto\nn\n. If\nm=n=1\n, we move\nv\n⃗ \n(l+1)\n3\nto the center point of the distance.\nv\n⃗ \n′\ni\n=\nτ\n3\n(\ndist(\nv\n⃗ \nμ\np\n,\nv\n⃗ \n(l+1)\ni\n)\n2\n) if m=n=1,\n(4)\nView Source\nwhere\nm\nand\nn\nare hyper-parameters and we discovered the optimal ratio value through a series of experiments with different ratios of\nm\nto\nn\n. This approach can make it easier for the decoder model in BART to learn distinct cluster patterns because it reduces the variance between vectors in a cluster by making the vectors in the cluster close together at a constant rate.\nSECTION IV.\nExperimental Set-Up\nFor the experiment, we first implemented the discriminator and the cluster generator using Python and then combined them into existing BART base model. To execute the BART model, we set 0.1 to dropout rate, 1 to batch size, 3e-5 to learning rate, 2 to gradient accumulation, and 0.1 to gradient clipping. As the hyper-parameters for summary generation, we set 6 to the number of beams, 62 to max length, 3 to max repeat\nn\n-gram, and TRUE to early stopping. We used 1,024 tokens as the maximum input size of BART.\nIn addition to the hyper-parameters of the existing BART model, the hyper-parameters of the proposed model are\nλ\n,\nw\n,\nm\n, and\nn\n, where\nλ\nis the number of salient sentences;\nw\nis the weight value for forming the cluster of salient sentences; and the ratio of\nm\nto\nn\nfor cluster shrinking. We selected best hyper-parameters through a series of experiments with different values.\nTo evaluate the proposed model for abstractive summarization, we used two distinct data sets: One is CNN/DailyMail [2] and the other is XSUM [35], which are main benchmark data sets in the text summarization field. It is known that the characteristics of the two data sets are slightly different. Reference summaries look like extractive style summaries on the CNN/DailyMail data set. On the other hand, in the XSUM data set, the reference summaries are extremely abstractive summaries, consisting of one or two sentences, as shown in Table 1.\nTABLE 1 One Example of Pairs of (Text Document, Reference Summary) in XSum Data Set [35]\nWe used ROUGE 1/2/L, which have been widely employed as the evaluation metric for text summarization [36]. Such metrics tend to estimate lexical similarity between two sequences by counting matched\nn\n-gram tokens between them. However, in case of tokens with same meaning but yet different spellings, such lexical-based similarity measures do not work effectively. To surmount this limitation, we also used BERTScore [37] to compute the semantic similarity between two sequences.\nAll models were in standalone executed in a high-performance workstation server with Intel Xeon Scalable Silver 4414 2.20GHz CPU with 40 cores, 24GB RAM, 1TB SSD, and GEFORCE RTX 3080 Ti D6 11GB BLOWER with 4,352 CUDA cores, 12GB RAM, and 7 GBPS memory clock.\nSECTION V.\nExperimental Results\nA. Comparison of Proposed Model to BART and PEGASUS\nWe first tested PEGASUS, BART, and the proposed model with XSUM and CNN/DailyMail data sets, respectively, and then measured ROUGE-1/2/L and BERTScore values. Tables 2 and 3 summarize the results in order to compare the performance of the proposed model with the existing BART and PEGASUS models. After we ran the proposed model with different hyper-parameters such as\nλ\n,\nw\n, and the ratio of\nm\nto\nn\nand chose them showing the best ROUGE and BERTScore, we obtained\nλ=0.5\n,\nw=−1.0\n,\nm=3\n, and\nn=1\nin XSUM data set and\nλ=0.6\n,\nw=−1.0\n,\nm=6\n, and\nn=1\nin CNN/DailyMail data set.\nTABLE 2 ROUGE-1/2/L/Avg and BERTScore Values of PEGASUS, BART and the Proposed Model in XSUM\nTABLE 3 ROUGE-1/2/L/Avg and BERTScore Values of PEGASUS, BART and the Proposed Model in CNN/DailyMail\nCompared with BART, the proposed model largely improves ROUGE-1/2/L/avg by 4%/5%/4%/4% and BERTScore by 0.4% in XSUM and improves ROUGE-1/2/L/avg by 2%/2%/2%/2% and BERTScore by 0.1% in CNN/DailyMail. Compared with PEGASUS, the proposed model largely improves ROUGE-1/L/avg by 8%/6%/4% and BERTScore by 0.8% in XSUM and improves ROUGE-1/L/avg by 8%/30%/10% and BERTScore by 0.7% in CNN/DailyMail. The reason why the proposed model shows higher performance than the existing BART and PEGASUS models is that the decoder generates an abstractive summary after learning cluster patterns made by the proposed cluster generator, which creates two clusters for salient and non-salient context vectors and increases the margin between the two clusters. For fine-tuning, the decoder is less affected by noise information such as non-salient context vectors, attending more over the cluster including salient context vectors. As a result, it can generate an abstractive summary including the main content of the original text.\nInterestingly, whatever the model is, all the ROUGE scores in the CNN/DailyMail data set are slightly higher than those in the XSUM data set. For example, the ROUGE-avg score of the proposed model is 27.72 in XSUM, whereas it is 33.62 in CNN/DailyMail. In contrast, all the BERTScore values in the CNN/DailyMail data set are slightly lower than those in the XSUM data set. For instance, the BERTScore value of the proposed model is 90.39 in XSUM, whereas it is 88.30 in CNN/DailyMail. This difference is due to the characteristics of the two data sets. Actually, most of the reference summaries of the XSUM data set were paraphrased from the original text as an extreme abstractive summary consisting of one or two sentences. On the other hand, each reference summary of the CNN/DailyMail data set is the set of salient sentences taken directly from the original text.\nAlthough the proposed model outperforms PEGASUS in overall performance, it is not better than PEGASUS in ROUGE-2. For example, as shown in Table 2, PEGASUS shows about 11% higher score than the proposed model. This is because there is a lot of overlap of bigram tokens between the generated summary and the reference summary. The strength of the proposed model based on BART lies in better understanding the overall context and maintaining consistency. In contrast, PEGASUS learns by masking important sentences and predicting them containing key information. For instance, here is an example.\nOriginal text: A major breakthrough in renewable energy technology was announced at the Global Tech Innovators Conference. Industry leaders presented a new solar panel that is twice as efficient as existing models and considerably cheaper to produce. Experts predict this innovation could revolutionize energy consumption worldwide.\nReference summary: At the Global Tech Conference, a new, more efficient, and cheaper solar panel was introduced that may change energy consumption globally.\nSummary generated by PEGASUS: The Tech Innovators Conference highlighted a solar panel breakthrough, promising to enhance energy efficiency and reduce costs.\nSummary generated by Proposed Model: At this year’s Global Tech Conference, scientists talked about global warming’s impact on ice caps and proposed new emission reduction strategies.\nIn the reference summary, key information is “more efficient, and cheaper solar panel”. The summary generated by PEGASUS contains key information about “solar panel breakthrough”, “energy efficiency”, and “reduce cost”, which connect “more efficient, and cheaper solar panel” well. However, key information like “global warming’s impact” in the summary generated by the proposed model disappears in the reference summary. This generated summary has partially similar meanings, but does not contain key information like the summary of PEGASUS.\nB. Results of Different Weight Values\nAs already discussed in Section III-D1, by scaling a context vector by a certain weight value\nw\n, context vectors corresponding to salient and non-salient sentences are clustered in the latent space. If\nw\nis between 0 and 1, the length of new vector\nv\n⃗ \n(l)\ni\nis reduced, compared to the given context vector\nv\n⃗ \ni\n, but the direction is the same. Furthermore, if\nw\nis greater than 1, the length of\nv\n⃗ \n(l)\ni\nincreases but the direction is also the same. On the other hand, if\nw\nis between 0 and -1, the length of\nv\n⃗ \n(l)\ni\nis small, indicating that it is in the opposite direction, compared to\nv\n⃗ \ni\n. If\nw\n= −1, new vector\nv\n⃗ \n(l)\ni\nhas the same length but the opposite direction.\nIf the cluster of salient sentence vectors is far from the cluster of non-salient ones in the latent space, the decoder in BART can focus mainly on learning the salient sentence vectors out of non-salient ones. For this reason, selecting an appropriate weight value\nw\nis important in the proposed model. In this experiment, we performed the proposed model with different weight values\nw\n= {−2.0, −1.5, −1.0, 1.5, 2.0} in the XSUM data set and measured its ROUGE-1/2/L and BERTScore values as summarized in Table 4.\nTABLE 4 ROUGE-1/2/L and BERTScore Values of the Proposed Model According to Different Weight Values\nWhen\nw\n= −1.0, the proposed model shows the best results in both ROUGE-1/2/L and BERTScore. These results are reasonable because of the relative position between the two clusters in the latent space. For any vector\nv\n⃗ \ni\n, the proposed model can generate the best summary when new vector\nv\n⃗ \n(l)\ni\nhas the same length as\nv\n⃗ \ni\nand is located in the opposite direction in the latent space. In other words, when\nw\n= −1.0, two clusters of salient and non-salient sentence vectors are well distinguished, but when\nw\n= 0.8, two clusters are close to each other, so they may not be well distinguished in the latent space. In contrast, the larger\nw\n, the farther the distance between two clusters becomes, so their relevance decreases and performance deteriorates. Note that all sentences consisting of a document are related to each other regardless of their importance.\nTo show statistical evidence for the proposed model, we prepared two proposed models\nm\n1\nand\nm\n2\n, which are learned after setting\nw=−1\n(best performance) and\nw=2\n(worse performance), respectively. We randomly selected 100 text documents from XSUM, where each document\nd\ni\nhas a reference summary\nRS(\nd\ni\n)\n. For\nd\ni\nin the sample,\nm\n1\noutputs a generated summary\nGS(\nd\ni\n)\n, and the score\nS(\nd\ni\n)\nbetween\nRS(\nd\ni\n)\nand\nGS(\nd\ni\n)\nis computed through ROUGE-avg or BERTScore. Note that\n0≤S(\nd\ni\n)≤1\n. Next, we created a frequency distribution table with 10 classes for all scores, using which each score is converted to the corresponding class mark. The same process as\nm\n1\nis performed for\nm\n2\n. Now there are two variables. One has 100 class marks to\nm\n1\nand the other has 100 class marks to\nm\n2\n. Finally, we performed the Fisher exact test between two variables [38], [39].\nNull hypothesis (\nH\n0\n): The two categorical variables are independent of each other. This means that the two summaries generated according to the different weight values are not related. This is, there is a difference in summary performance.\nAlternative hypothesis (\nH\n1\n): Two categorical variables are not independent of each other. This means that the two summaries generated according to the different weight values are related. This is, there is no difference in summary performance.\nThe Fisher exact test results show that\nH\n0\nis accepted because the p-value of ROUGE-avg is\n0.6832≫0.05\nand the p-value of BERTScore is\n0.4562≫0.05\n. Consequently, we conclude that there is a statistical difference in summaries generated according to the different weight values in the proposed model.\nC. Results of Number of Salient Sentences\nAs an input to the discriminator\nΔ\n,\nλ\n, one of the hyper-parameters in the proposed model, controls the number of salient sentences. Supposing an original text\nd={\ns\n1\n,\ns\n2\n,…,\ns\n10\n}\n,\nΔ\nextracts three sentences as salient sentences in\nd\nif we set\nλ\nto 0.3. In this work, we evaluated how well the proposed model generates a summary according to the number of salient sentences per\nd\n. For this, we experimented the proposed model with different\nλ\nvalues in the XSUM data set and compared the results of ROUGE-1/2/L and BERTScore.\nTable 5 shows the results according to different\nλ\nvalues. In this experiment, we used\nw\n= −1.0 and the ratio of\nm=3\nto\nn=1\nin cluster shrinking. The experimental results are similar to each other. However, when\nλ=0.2\n, the ROUGE-1/2/L score is marginally better, and when\nλ=0.5\n, the BERTScore value is slightly higher than the others. Overall, the ROUGE and BERTScore values seem to increase slightly until\nλ=0.5\n, and then, from\nλ=0.5\n, those values tend to decrease slightly. From these experimental results, we can figure out that\nλ=0.5\nis better for summary generation. Similar results were also obtained for the CNN/DailyMail data set, which is omitted here due to space limitations.\nTABLE 5 ROUGE-1/2/L and BERTScore Values of the Proposed Model According to Different\nλ\nValues\nD. Results of Ratio of\nm\n:\nn\nfor Cluster Shrinking\nTo reduce the size of clusters, we first compute the mean vector in each cluster and then find the distance between the mean vector and any vector belonging to the cluster. Subsequently, we divide this distance by the ratio of\nm\nto\nn\nand move the existing vector to the center position between\nm\nand\nn\n. The purpose of this approach is to distinguish salient sentence vectors from non-salient ones well. The size of clusters formed in the latent space can be reduced by maximizing the margin between the two clusters of salient and non-salient vectors. Through this clustering shrinking method, the decoder in BART are likely to learn cluster patterns easily.\nTable 6 shows the results of the proposed model according to different ratios of\nm\nto\nn\nvalues, where\nw\nand\nλ\nare set to −1.0 and 0.5. In the XSUM data set, both the ROUGE-1/2/L and BERTScore values are the highest when\nm=3\nand\nn=1\n. On the other hand, in the case of the CNN/DailyMail data set, best ROUGE-1/2/L and BERTScore results are shown when the ratio of\nm=6\nto\nn=1\n. For instance, the ROUGE-1/2/L and BERTScore values of the proposed model are 41.99, 20.01, 38.85, and 88.30, respectively. This indicates that the ratio of\nm\nto\nn\ndepends on the characteristics of the given summarization data set for cluster shrinking.\nTABLE 6 ROUGE-1/2/L and BERTScore Values of the Proposed Model According to Different Ratios of\nm\nto\nn\nValues\nE. Discussion of Generated Summaries\nTable 7 shows two abstractive summaries generated by both BART and the proposed model. The actual original text has a long text span, but due to space limitations, we omitted a significant amount of text in the table. As the reference summary, the main point summarized by a human evaluator is that US tennis star Venus Williams was involved in a car accident with the death of a man. Both summaries generated by BART and the proposed model focus on a car accident with a famous tennis player in US. The BART and proposed models all summarized the original text well as a human-written abstract usually does. In addition, the generated summaries are natural, fluent, and no errors in grammar.\nTABLE 7 A Case Study of Summaries Generated by BART and the Proposed Model\nHowever, the BART model summarized incorrectly, while the proposed model did correctly. The US tennis star is not Serena Williams but Venus Williams in the car accident. As we can see from these results, learning distinct cluster patterns in the latent space prevents the sequence-to-sequence model for abstractive summarization from inaccurately reproducing factual details. This is because the summary is generated by referring to only salient sentences that are well representative of the given text document. In addition, these results might give us an indirect evidence in that the\nΔ\nand\nτ\nof our proposed method work well.\nSECTION VI.\nConclusion\nTo improve abstractive summarization, we propose a new pre-trained sequence-to-sequence model containing a discriminator and a cluster generator between the encoder and the decoder. During fine-tuning, the discriminator extracts salient sentences from a given text document. The cluster generator generates two clusters of salient and non-salient context vectors from the encoder, and normalizes and shrinks the clusters to better distinguish the two clusters. Our experimental results show that the proposed method outperforms the existing BART and PEGASUS models in two summarization benchmark data sets.\nFor the future research direction, we will further refine our proposed model in order to tackle the fact inconsistency (hallucination) problem between input document and generated summary and to generate abstractive summaries for long documents such as scientific papers, reports, and books.\nACKNOWLEDGMENT\nThe authors would like to thank the anonymous reviewers for their feedback.', web_link='https://ieeexplore.ieee.org/document/10373873/', local_link=None, other_data={'authors': ['Sung-Guk Jo', 'Seung-Hyeok Park', 'Jeong-Jae Kim', 'Byung-Won On']}, pub_date=datetime.datetime(2023, 12, 25, 0, 0, tzinfo=pytz.UTC), load_date=None)

categories = ('Computational and artificial intelligence',
              'Computers and information processing',
              'Communications technology',
              'Industry applications',
              'Vehicular and wireless technologies',
              'Systems engineering and theory',
              'Intelligent transportation systems',
              'Information theory',
              'Electronic design automation and methodology',
              'Education',
              'Social implications of technology')

url = 'https://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=10005208&punumber=6287639&sortType=vol-only-newest'


parser = IEEE(driver(), url, categories, 10, doc)
docs: list[SPP_document] = parser.content()

print(*docs, sep='\n\r\n')
